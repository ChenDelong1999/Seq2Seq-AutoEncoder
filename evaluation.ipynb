{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "from data.dataset import get_dataset, SeqMaskDataset, LVISDataset, V3DetDataset, COCODataset, VisualGenomeDataset\n",
    "from model import Seq2SeqAutoEncoderModel\n",
    "\n",
    "model_dir = '/home/dchenbs/workspace/Seq2Seq-AutoEncoder/runs/Nov14_17-31-06_host19-SA1B-[327MB-16queries-1024]-[lr1e-05-bs16x1step-8gpu]/checkpoints/checkpoint_ep0_step350k'\n",
    "model = Seq2SeqAutoEncoderModel.from_pretrained(model_dir).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image_from_data(data, width, height, num_queries, img_channels=3):\n",
    "    data = data.numpy()\n",
    "    segment_data = data[1:, :3]\n",
    "    shape_encoding_seq = data[1:, 3]\n",
    "    is_data_seq = data[1:, 4]\n",
    "\n",
    "    shape_encoding_seq = shape_encoding_seq > 0.3\n",
    "    is_data_seq = is_data_seq > 0.5\n",
    "\n",
    "    # find the last positive element in shape_encoding_seq, and use it to truncate data and sequences\n",
    "    last_positive_index = np.nonzero(shape_encoding_seq)[0][-1]\n",
    "    shape_encoding_seq = shape_encoding_seq[:last_positive_index+1]\n",
    "    is_data_seq = is_data_seq[:last_positive_index+1]\n",
    "    segment_data = segment_data[:last_positive_index+1] * 255\n",
    "\n",
    "    # height is the number of non zero element in shape_encoding_seq\n",
    "    height_decoded = np.sum(shape_encoding_seq)\n",
    "\n",
    "    # width is the largest interval between two consecutive non zero elements in shape_encoding_seq\n",
    "    width_decoded = 0\n",
    "    true_indices = np.where(shape_encoding_seq)[0]\n",
    "    true_indices = np.insert(true_indices, 0, 0)\n",
    "    diffs = np.diff(true_indices)\n",
    "    if diffs.size > 0:\n",
    "        width_decoded = np.max(diffs)\n",
    "\n",
    "    width_decoded += 1 # don't know why, but fix bug\n",
    "\n",
    "    segment = np.zeros((height_decoded, width_decoded, 3))\n",
    "    mask = np.zeros((height_decoded, width_decoded))\n",
    "\n",
    "    # split segment_data into parts according to shape_encoding_seq=True positions, splited parts could be in different length\n",
    "    split_indices = np.where(shape_encoding_seq)[0]\n",
    "    split_indices += 1\n",
    "    split_segment_data = np.split(segment_data, split_indices)\n",
    "    split_segment_data = [x for x in split_segment_data if len(x) > 0]\n",
    "\n",
    "    split_is_data = np.split(is_data_seq, split_indices)\n",
    "    split_is_data = [x for x in split_is_data if len(x) > 0]\n",
    "\n",
    "    for row_id in range(len(split_segment_data)):\n",
    "        segment_split = split_segment_data[row_id]\n",
    "        mask_split = split_is_data[row_id]\n",
    "        segment[row_id, :len(segment_split), :] = segment_split\n",
    "        mask[row_id, :len(mask_split)] = mask_split\n",
    "    \n",
    "    # apply mask to the segment: set all masked pixels to 255\n",
    "    segment[mask == 0] = 255  \n",
    "    segment = segment[:, :-1, :].astype(np.uint8)\n",
    "    segment = transforms.ToPILImage()(segment)\n",
    "\n",
    "    return segment, np.array(is_data_seq), np.array(shape_encoding_seq)\n",
    "\n",
    "\n",
    "def visualize_segments(sample_info, original_segment, reconstructed_segment):\n",
    "\n",
    "    if 'image_path' in sample_info.keys():\n",
    "        fig, ax = plt.subplots(1, 3)\n",
    "        fig.set_size_inches(12, 4)\n",
    "        fig.suptitle(sample_info['name'])\n",
    "        ax[0].imshow(Image.open(sample_info['image_path']))\n",
    "        x, y, w, h = sample_info['bbox']\n",
    "        rect = plt.Rectangle((x, y), w, h, fill=False, color='red')\n",
    "        ax[0].add_patch(rect)\n",
    "        ax[1].imshow(original_segment)\n",
    "        ax[2].imshow(reconstructed_segment)\n",
    "    else:\n",
    "        fig, ax = plt.subplots(1, 2)\n",
    "        ax[0].imshow(original_segment)\n",
    "        ax[1].imshow(reconstructed_segment)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_root = '/home/dchenbs/workspace/datasets/coco2017'\n",
    "coco_dataset = SeqMaskDataset(\n",
    "    dataset=COCODataset(coco_root=coco_root, split='val'), \n",
    "    num_queries=model.config.num_queries, \n",
    "    virtual_dataset_size=244707, \n",
    "    data_seq_length=model.config.data_seq_length,\n",
    "    min_resize_ratio=1,\n",
    ")\n",
    "\n",
    "lvis_root = '/home/dchenbs/workspace/datasets/lvis'\n",
    "coco_root = '/home/dchenbs/workspace/datasets/coco2017'\n",
    "lvis_dataset = SeqMaskDataset(\n",
    "    dataset=LVISDataset(lvis_root=lvis_root, coco_root=coco_root, split='val'), \n",
    "    num_queries=model.config.num_queries, \n",
    "    virtual_dataset_size=244707, \n",
    "    data_seq_length=model.config.data_seq_length,\n",
    "    min_resize_ratio=1,\n",
    ")\n",
    "\n",
    "v3det_root = '/home/dchenbs/workspace/datasets/v3det'\n",
    "v3det_dataset = SeqMaskDataset(\n",
    "    dataset=V3DetDataset(v3det_root=v3det_root, split='val'), \n",
    "    num_queries=model.config.num_queries, \n",
    "    virtual_dataset_size=244707, \n",
    "    data_seq_length=model.config.data_seq_length,\n",
    "    min_resize_ratio=1,\n",
    ")\n",
    "\n",
    "visual_genome_root = '/home/dchenbs/workspace/datasets/VisualGenome'\n",
    "visual_genome_dataset = SeqMaskDataset(\n",
    "    dataset=VisualGenomeDataset(visual_genome_root=visual_genome_root, split='val'), \n",
    "    num_queries=model.config.num_queries, \n",
    "    virtual_dataset_size=244707, \n",
    "    data_seq_length=model.config.data_seq_length,\n",
    "    min_resize_ratio=1,\n",
    ")\n",
    "\n",
    "for dataset in [coco_dataset, lvis_dataset, v3det_dataset, visual_genome_dataset]:\n",
    "    for _ in range(1):\n",
    "        batch_size = 1\n",
    "        batch_data = []\n",
    "        batch_sample_info = []\n",
    "        for i in range(batch_size):\n",
    "            for j in range(50):\n",
    "                index = np.random.randint(0, len(dataset))\n",
    "                this_data, this_sample_info = dataset[index]\n",
    "            batch_data.append(this_data)\n",
    "            batch_sample_info.append(this_sample_info)\n",
    "\n",
    "        batch_data = torch.stack(batch_data).cuda()\n",
    "        batch_latents = model.encode(batch_data)\n",
    "        batch_reconstructed = model.generate(batch_latents, show_progress_bar=True)\n",
    "\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            data = batch_data[i]\n",
    "            reconstructed = batch_reconstructed[i]\n",
    "            sample_info = batch_sample_info[i]\n",
    "\n",
    "            original_segment, original_is_data, original_shape_encoding = decode_image_from_data(\n",
    "                data.cpu(), \n",
    "                sample_info['width'], \n",
    "                sample_info['height'], \n",
    "                dataset.num_queries, \n",
    "                img_channels=dataset.img_channels\n",
    "                )\n",
    "            reconstructed_segment, reconstructed_is_data, reconstructed_shape_encoding = decode_image_from_data(\n",
    "                reconstructed.cpu(), \n",
    "                sample_info['width'], \n",
    "                sample_info['height'], \n",
    "                dataset.num_queries, \n",
    "                img_channels=dataset.img_channels\n",
    "                )\n",
    "\n",
    "            fig = visualize_segments(sample_info, original_segment, reconstructed_segment)\n",
    "            print(f\"[{dataset.dataset.dataset_name}]: {sample_info['caption']}\")\n",
    "\n",
    "            plt.show()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq2seq-ae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
