{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import pprint\n",
    "\n",
    "rank = 4\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(rank)\n",
    "\n",
    "model_dirs = [\n",
    "    # '/home/dchenbs/workspace/Seq2Seq-AutoEncoder/runs/Nov14_17-31-06_host19-SA1B-[327MB-16queries-1024]-[lr1e-05-bs16x1step-8gpu]/checkpoints/checkpoint_ep0_step50k',\n",
    "    # '/home/dchenbs/workspace/Seq2Seq-AutoEncoder/runs/Nov14_17-31-06_host19-SA1B-[327MB-16queries-1024]-[lr1e-05-bs16x1step-8gpu]/checkpoints/checkpoint_ep0_step100k',\n",
    "    # '/home/dchenbs/workspace/Seq2Seq-AutoEncoder/runs/Nov14_17-31-06_host19-SA1B-[327MB-16queries-1024]-[lr1e-05-bs16x1step-8gpu]/checkpoints/checkpoint_ep0_step150k',\n",
    "    # '/home/dchenbs/workspace/Seq2Seq-AutoEncoder/runs/Nov14_17-31-06_host19-SA1B-[327MB-16queries-1024]-[lr1e-05-bs16x1step-8gpu]/checkpoints/checkpoint_ep0_step200k',\n",
    "    # '/home/dchenbs/workspace/Seq2Seq-AutoEncoder/runs/Nov14_17-31-06_host19-SA1B-[327MB-16queries-1024]-[lr1e-05-bs16x1step-8gpu]/checkpoints/checkpoint_ep0_step250k',\n",
    "    # '/home/dchenbs/workspace/Seq2Seq-AutoEncoder/runs/Nov14_17-31-06_host19-SA1B-[327MB-16queries-1024]-[lr1e-05-bs16x1step-8gpu]/checkpoints/checkpoint_ep0_step300k',\n",
    "    # '/home/dchenbs/workspace/Seq2Seq-AutoEncoder/runs/Nov14_17-31-06_host19-SA1B-[327MB-16queries-1024]-[lr1e-05-bs16x1step-8gpu]/checkpoints/checkpoint_ep0_step350k',\n",
    "    # '/home/dchenbs/workspace/Seq2Seq-AutoEncoder/runs/Nov14_17-31-06_host19-SA1B-[327MB-16queries-1024]-[lr1e-05-bs16x1step-8gpu]/checkpoints/checkpoint_ep0_step400k',\n",
    "    # '/home/dchenbs/workspace/Seq2Seq-AutoEncoder/runs/Nov14_17-31-06_host19-SA1B-[327MB-16queries-1024]-[lr1e-05-bs16x1step-8gpu]/checkpoints/checkpoint_ep0_step450k',\n",
    "    # '/home/dchenbs/workspace/Seq2Seq-AutoEncoder/runs/Nov14_17-31-06_host19-SA1B-[327MB-16queries-1024]-[lr1e-05-bs16x1step-8gpu]/checkpoints/checkpoint_ep0_step500k',\n",
    "    # '/home/dchenbs/workspace/Seq2Seq-AutoEncoder/runs/Nov14_17-31-06_host19-SA1B-[327MB-16queries-1024]-[lr1e-05-bs16x1step-8gpu]/checkpoints/checkpoint_ep0_step550k',\n",
    "    # '/home/dchenbs/workspace/Seq2Seq-AutoEncoder/runs/Nov14_17-31-06_host19-SA1B-[327MB-16queries-1024]-[lr1e-05-bs16x1step-8gpu]/checkpoints/checkpoint_ep0_step600k',\n",
    "    # '/home/dchenbs/workspace/Seq2Seq-AutoEncoder/runs/Nov14_17-31-06_host19-SA1B-[327MB-16queries-1024]-[lr1e-05-bs16x1step-8gpu]/checkpoints/checkpoint_ep0_step650k',\n",
    "    # '/home/dchenbs/workspace/Seq2Seq-AutoEncoder/runs/Nov14_17-31-06_host19-SA1B-[327MB-16queries-1024]-[lr1e-05-bs16x1step-8gpu]/checkpoints/checkpoint_ep0_step700k',\n",
    "    # '/home/dchenbs/workspace/Seq2Seq-AutoEncoder/runs/Nov14_17-31-06_host19-SA1B-[327MB-16queries-1024]-[lr1e-05-bs16x1step-8gpu]/checkpoints/checkpoint_ep0_step750k',\n",
    "    # '/home/dchenbs/workspace/Seq2Seq-AutoEncoder/runs/Nov14_17-31-06_host19-SA1B-[327MB-16queries-1024]-[lr1e-05-bs16x1step-8gpu]/checkpoints/checkpoint_ep0_step800k',\n",
    "    # '/home/dchenbs/workspace/Seq2Seq-AutoEncoder/runs/Nov14_17-31-06_host19-SA1B-[327MB-16queries-1024]-[lr1e-05-bs16x1step-8gpu]/checkpoints/checkpoint_ep0_step850k',\n",
    "    '/home/dchenbs/workspace/Seq2Seq-AutoEncoder/runs/Nov14_17-31-06_host19-SA1B-[327MB-16queries-1024]-[lr1e-05-bs16x1step-8gpu]/checkpoints/checkpoint_ep0_step900k',\n",
    "    ]\n",
    "\n",
    "# model_dirs = model_dirs[rank*2 : (rank+1)*2]\n",
    "pprint.pprint(model_dirs)\n",
    "\n",
    "from data.dataset import get_dataset, SeqMaskDataset, LVISDataset, V3DetDataset, COCODataset, VisualGenomeDataset, SA1BDataset\n",
    "from model import Seq2SeqAutoEncoderModel, Seq2SeqAutoEncoderConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image_from_seq(seq):\n",
    "    seq = seq.numpy()\n",
    "    segment_data = seq[1:, :3]\n",
    "    shape_encoding_seq = seq[1:, 3]\n",
    "    is_data_seq = seq[1:, 4]\n",
    "\n",
    "    shape_encoding_seq = shape_encoding_seq > 0.3\n",
    "    is_data_seq = is_data_seq > 0.5\n",
    "\n",
    "    # find the last positive element in shape_encoding_seq, and use it to truncate data and sequences\n",
    "    last_positive_index = np.nonzero(shape_encoding_seq)[0][-1]\n",
    "    shape_encoding_seq = shape_encoding_seq[:last_positive_index+1]\n",
    "    is_data_seq = is_data_seq[:last_positive_index+1]\n",
    "    segment_data = segment_data[:last_positive_index+1] * 255\n",
    "\n",
    "    # height is the number of non zero element in shape_encoding_seq\n",
    "    height_decoded = np.sum(shape_encoding_seq)\n",
    "\n",
    "    # width is the largest interval between two consecutive non zero elements in shape_encoding_seq\n",
    "    width_decoded = 0\n",
    "    true_indices = np.where(shape_encoding_seq)[0]\n",
    "    true_indices = np.insert(true_indices, 0, 0)\n",
    "    diffs = np.diff(true_indices)\n",
    "    if diffs.size > 0:\n",
    "        width_decoded = np.max(diffs)\n",
    "\n",
    "    width_decoded += 1 # don't know why, but fix bug\n",
    "\n",
    "    segment = np.zeros((height_decoded, width_decoded, 3))\n",
    "    mask = np.zeros((height_decoded, width_decoded))\n",
    "\n",
    "    # split segment_data into parts according to shape_encoding_seq=True positions, splited parts could be in different length\n",
    "    split_indices = np.where(shape_encoding_seq)[0]\n",
    "    split_indices += 1\n",
    "    split_segment_data = np.split(segment_data, split_indices)\n",
    "    split_segment_data = [x for x in split_segment_data if len(x) > 0]\n",
    "\n",
    "    split_is_data = np.split(is_data_seq, split_indices)\n",
    "    split_is_data = [x for x in split_is_data if len(x) > 0]\n",
    "\n",
    "    for row_id in range(len(split_segment_data)):\n",
    "        segment_split = split_segment_data[row_id]\n",
    "        mask_split = split_is_data[row_id]\n",
    "        segment[row_id, :len(segment_split), :] = segment_split\n",
    "        mask[row_id, :len(mask_split)] = mask_split\n",
    "    \n",
    "    # apply mask to the segment: set all masked pixels to 255\n",
    "    segment[mask == 0] = 255  \n",
    "    segment = segment[:, :-1, :].astype(np.uint8)\n",
    "    segment = transforms.ToPILImage()(segment)\n",
    "\n",
    "    return segment, np.array(is_data_seq), np.array(shape_encoding_seq)\n",
    "\n",
    "\n",
    "def visualize_segments(sample_info, original_segment, reconstructed_segment):\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3)\n",
    "    fig.set_size_inches(15, 5)\n",
    "    fig.suptitle(sample_info['name'])\n",
    "    ax[0].imshow(Image.open(sample_info['image_path']))\n",
    "    x, y, w, h = sample_info['bbox']\n",
    "    rect = plt.Rectangle((x, y), w, h, fill=False, color='red')\n",
    "    ax[0].add_patch(rect)\n",
    "    ax[1].imshow(original_segment)\n",
    "    ax[2].imshow(reconstructed_segment)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vis_dir = 'visualizations'\n",
    "if not os.path.exists(vis_dir):\n",
    "    os.makedirs(vis_dir)\n",
    "\n",
    "for model_dir in model_dirs:\n",
    "    print('# '*64)\n",
    "    print(f'Loading model from {model_dir}')\n",
    "    model = Seq2SeqAutoEncoderModel.from_pretrained(model_dir).cuda().eval()\n",
    "\n",
    "    # sa1b_root = '/home/dchenbs/workspace/datasets/sa1b'\n",
    "    # sa1b_dataset = SeqMaskDataset(\n",
    "    #     dataset=SA1BDataset(sa1b_root=sa1b_root), \n",
    "    #     num_queries=model.config.num_queries, \n",
    "    #     data_seq_length=model.config.data_seq_length,\n",
    "    # )\n",
    "\n",
    "    coco_root = '/home/dchenbs/workspace/datasets/coco2017'\n",
    "    coco_dataset = SeqMaskDataset(\n",
    "        dataset=COCODataset(coco_root=coco_root, split='val'), \n",
    "        num_queries=model.config.num_queries, \n",
    "        data_seq_length=model.config.data_seq_length,\n",
    "    )\n",
    "\n",
    "    lvis_root = '/home/dchenbs/workspace/datasets/lvis'\n",
    "    coco_root = '/home/dchenbs/workspace/datasets/coco2017'\n",
    "    lvis_dataset = SeqMaskDataset(\n",
    "        dataset=LVISDataset(lvis_root=lvis_root, coco_root=coco_root, split='val'), \n",
    "        num_queries=model.config.num_queries, \n",
    "        data_seq_length=model.config.data_seq_length,\n",
    "    )\n",
    "\n",
    "    v3det_root = '/home/dchenbs/workspace/datasets/v3det'\n",
    "    v3det_dataset = SeqMaskDataset(\n",
    "        dataset=V3DetDataset(v3det_root=v3det_root, split='val'), \n",
    "        num_queries=model.config.num_queries, \n",
    "        data_seq_length=model.config.data_seq_length,\n",
    "    )\n",
    "\n",
    "    visual_genome_root = '/home/dchenbs/workspace/datasets/VisualGenome'\n",
    "    visual_genome_dataset = SeqMaskDataset(\n",
    "        dataset=VisualGenomeDataset(visual_genome_root=visual_genome_root, split='val'), \n",
    "        num_queries=model.config.num_queries, \n",
    "        data_seq_length=model.config.data_seq_length,\n",
    "    )\n",
    "\n",
    "    num_steps = 100\n",
    "    batch_size = 50\n",
    "    for dataset in [coco_dataset, lvis_dataset, v3det_dataset, visual_genome_dataset]:\n",
    "    # for dataset in [sa1b_dataset, coco_dataset, lvis_dataset, v3det_dataset, visual_genome_dataset]:\n",
    "        all_latent = []\n",
    "        all_sample_info = []\n",
    "        print(f'Generating latent vectors for {dataset.dataset.dataset_name} dataset')\n",
    "        for step in tqdm.tqdm(range(num_steps)):\n",
    "            batch_data = []\n",
    "            for i in range(batch_size):\n",
    "                this_data, this_sample_info = dataset[np.random.randint(0, len(dataset))]\n",
    "                batch_data.append(this_data)\n",
    "                this_sample_info['class_id'] = dataset.dataset.class_name_to_class_id(this_sample_info['name'])\n",
    "                all_sample_info.append(this_sample_info)\n",
    "            batch_data = torch.stack(batch_data).cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_latents = model.encode(batch_data).cpu().numpy()\n",
    "            all_latent.append(batch_latents)\n",
    "        \n",
    "        all_latent = np.concatenate(all_latent, axis=0)\n",
    "        all_ids = np.array([x['class_id'] for x in all_sample_info])\n",
    "\n",
    "\n",
    "        from sklearn.manifold import TSNE\n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        latents_tsne = tsne.fit_transform(all_latent)\n",
    "\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        plt.scatter(latents_tsne[:, 0], latents_tsne[:, 1], c=all_ids, cmap='tab20', s=10, alpha=0.5)\n",
    "        plt.axis('off')\n",
    "        plt.title(f'T-SNE of {len(all_ids)} samples in {dataset.dataset.dataset_name.upper()} dataset ({dataset.dataset.num_categories} categories)')\n",
    "        plt.savefig(os.path.join(vis_dir, f'tsne-{dataset.dataset.dataset_name}-{model_dir.split(\"/\")[-1]}.png'), bbox_inches='tight', pad_inches=0)\n",
    "        plt.show()\n",
    "\n",
    "    num_steps = 1\n",
    "    batch_size = 8\n",
    "    for dataset in [coco_dataset, lvis_dataset, v3det_dataset, visual_genome_dataset]:\n",
    "        print(f'Generating reconstructions for {dataset.dataset.dataset_name} dataset')\n",
    "        for step in range(num_steps):\n",
    "            batch_data = []\n",
    "            batch_sample_info = []\n",
    "            for i in range(batch_size):\n",
    "                for j in range(50):\n",
    "                    index = np.random.randint(0, len(dataset))\n",
    "                    this_data, this_sample_info = dataset[index]\n",
    "                batch_data.append(this_data)\n",
    "                batch_sample_info.append(this_sample_info)\n",
    "\n",
    "            batch_data = torch.stack(batch_data).cuda()\n",
    "            batch_latents = model.encode(batch_data)\n",
    "            batch_reconstructed = model.generate(batch_latents, show_progress_bar=True)\n",
    "\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                seq = batch_data[i]\n",
    "                reconstructed = batch_reconstructed[i]\n",
    "                sample_info = batch_sample_info[i]\n",
    "\n",
    "                original_segment, original_is_data, original_shape_encoding = decode_image_from_seq(seq.cpu())\n",
    "                reconstructed_segment, reconstructed_is_data, reconstructed_shape_encoding = decode_image_from_seq(reconstructed.cpu())\n",
    "\n",
    "                fig = visualize_segments(sample_info, original_segment, reconstructed_segment)\n",
    "                print(f\"[{dataset.dataset.dataset_name}]: {sample_info['caption']}\")\n",
    "                plt.savefig(os.path.join(vis_dir, f'reconstructions-{dataset.dataset.dataset_name}-{model_dir.split(\"/\")[-1]}-{batch_size*step+i}.png'), bbox_inches='tight', pad_inches=0)\n",
    "                plt.show()\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq2seq-ae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
