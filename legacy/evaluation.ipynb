{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import pprint\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "from model import Seq2SeqAutoEncoderModel, Seq2SeqAutoEncoderConfig\n",
    "from evaluation import decode_image_from_seq, visualize_segments, tsne_visualize, get_knn_similarity, get_datasets, linear_evaluation, reconstruction_evaluation\n",
    "\n",
    "\n",
    "from data.dataset import get_dataset, SeqMaskDataset, LVISDataset, V3DetDataset, COCODataset, VisualGenomeDataset, SA1BDataset\n",
    "\n",
    "def get_datasets(model, expand_mask_ratio=0):\n",
    "\n",
    "    coco_dataset = SeqMaskDataset(\n",
    "        dataset=COCODataset(coco_root='/home/dchenbs/workspace/datasets/coco2017', split='val'), \n",
    "        num_queries=model.config.num_queries, data_seq_length=model.config.data_seq_length,\n",
    "        text_features='data/text_features/coco_clip_rn50.npy',\n",
    "        expand_mask_ratio=expand_mask_ratio,\n",
    "    )\n",
    "\n",
    "    lvis_dataset = SeqMaskDataset(\n",
    "        dataset=LVISDataset(lvis_root='/home/dchenbs/workspace/datasets/lvis', coco_root='/home/dchenbs/workspace/datasets/coco2017', split='val'), \n",
    "        num_queries=model.config.num_queries, data_seq_length=model.config.data_seq_length,\n",
    "        text_features='data/text_features/lvis_clip_rn50.npy',\n",
    "        expand_mask_ratio=expand_mask_ratio,\n",
    "    )\n",
    "\n",
    "    v3det_dataset = SeqMaskDataset(\n",
    "        dataset=V3DetDataset(v3det_root='/home/dchenbs/workspace/datasets/v3det', split='val'), \n",
    "        num_queries=model.config.num_queries, data_seq_length=model.config.data_seq_length,\n",
    "        text_features='data/text_features/v3det_clip_rn50.npy',\n",
    "        expand_mask_ratio=expand_mask_ratio,\n",
    "    )\n",
    "\n",
    "    return [coco_dataset, lvis_dataset, v3det_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "# model_dir = '/home/dchenbs/workspace/Seq2Seq-AutoEncoder/runs/Nov28_20-50-04_host19-SA1B-[327MB-16queries-1024]-[lr1e-05-bs16x1step-8gpu]/checkpoints/checkpoint_ep0_step1000k'\n",
    "\n",
    "# print(f'Loading model from {model_dir}')\n",
    "\n",
    "# model = Seq2SeqAutoEncoderModel.from_pretrained(model_dir).half().cuda().eval()\n",
    "# datasets = get_datasets(model)\n",
    "\n",
    "# num_steps = 4\n",
    "# batch_size = 2\n",
    "\n",
    "# for dataset in datasets[1:]:\n",
    "#     print(f'Generating reconstructions for {dataset.dataset.dataset_name} dataset')\n",
    "#     save_dir = os.path.join(model_dir, 'reconstructions')\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "#     for step in range(num_steps):\n",
    "#         batch_data = []\n",
    "#         batch_sample_info = []\n",
    "#         for i in range(batch_size):\n",
    "#             success = False\n",
    "#             while not success:\n",
    "#                 this_data, this_sample_info = dataset[np.random.randint(0, len(dataset))]        \n",
    "\n",
    "#                 img = Image.open(this_sample_info['image_path'])\n",
    "#                 aspect_ratio = img.size[0] / img.size[1]\n",
    "#                 img_area = img.size[0] * img.size[1]\n",
    "#                 if aspect_ratio < 1:\n",
    "#                     aspect_ratio = 1 / aspect_ratio\n",
    "#                 area = this_sample_info['bbox'][2] * this_sample_info['bbox'][3]\n",
    "#                 if area > 0.33*img_area and area < 0.66*img_area and aspect_ratio < 1.5:\n",
    "#                     success = True\n",
    "\n",
    "#             batch_data.append(this_data)\n",
    "#             batch_sample_info.append(this_sample_info)\n",
    "#             dataset.sample_buffer = []\n",
    "\n",
    "#         batch_data = torch.stack(batch_data).half().cuda()\n",
    "#         batch_latents = model.encode(batch_data)\n",
    "#         batch_reconstructed = model.generate(batch_latents, show_progress_bar=True)\n",
    "\n",
    "#         for i in range(batch_size):\n",
    "#             seq = batch_data[i]\n",
    "#             reconstructed = batch_reconstructed[i]\n",
    "#             sample_info = batch_sample_info[i]\n",
    "\n",
    "#             original_segment, original_mask = decode_image_from_seq(seq.float().cpu().numpy())\n",
    "#             reconstructed_segment, reconstructed_mask = decode_image_from_seq(reconstructed.float().cpu().numpy())\n",
    "\n",
    "#             print(f\"[{dataset.dataset.dataset_name}]: {sample_info['caption']}\")\n",
    "#             fig = visualize_segments(sample_info, original_segment, reconstructed_segment)\n",
    "            \n",
    "#             img_name = f'{dataset.dataset.dataset_name}_{step*batch_size + i}.png'\n",
    "#             plt.savefig(os.path.join(save_dir, img_name), bbox_inches='tight')\n",
    "#             # plt.show()\n",
    "#             plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "model_configs = [\n",
    "    'runs/Nov14_17-31-06_host19-SA1B-[327MB-16queries-1024]-[lr1e-05-bs16x1step-8gpu]/checkpoints/checkpoint_ep0_step50k/config.json',\n",
    "    'runs/Nov14_17-31-06_host19-SA1B-[327MB-16queries-1024]-[lr1e-05-bs16x1step-8gpu]/checkpoints/checkpoint_ep0_step50k',\n",
    "    'runs/Nov14_17-31-06_host19-SA1B-[327MB-16queries-1024]-[lr1e-05-bs16x1step-8gpu]/checkpoints/checkpoint_ep0_step500k',\n",
    "    'runs/Nov14_17-31-06_host19-SA1B-[327MB-16queries-1024]-[lr1e-05-bs16x1step-8gpu]/checkpoints/checkpoint_ep0_step1000k',\n",
    "    'runs/Nov28_20-50-04_host19-SA1B-[327MB-16queries-1024]-[lr1e-05-bs16x1step-8gpu]/checkpoints/checkpoint_ep0_step500k',\n",
    "    'runs/Nov28_20-50-04_host19-SA1B-[327MB-16queries-1024]-[lr1e-05-bs16x1step-8gpu]/checkpoints/checkpoint_ep0_step1000k',\n",
    "    ]\n",
    "features = {}\n",
    "num_steps=250\n",
    "batch_size=100\n",
    "\n",
    "for model_config in model_configs:\n",
    "    if model_config.endswith('.json'):\n",
    "        config = Seq2SeqAutoEncoderConfig.from_json_file(model_config)\n",
    "        model = Seq2SeqAutoEncoderModel(config)\n",
    "    else:\n",
    "        model = Seq2SeqAutoEncoderModel.from_pretrained(model_config)\n",
    "    model = model.half().cuda().eval()\n",
    "    print(f'>>> Loaded model from {model_config}')\n",
    "\n",
    "    features[model_config] = {}\n",
    "\n",
    "    datasets = get_datasets(model, expand_mask_ratio=0)[1:2] # only coco\n",
    "    for dataset in datasets:\n",
    "        all_latents = []\n",
    "        all_sample_info = []\n",
    "        print(f'Generating latent vectors for {dataset.dataset.dataset_name} dataset')\n",
    "        for step in tqdm.tqdm(range(num_steps)):\n",
    "            batch_data = []\n",
    "            for i in range(batch_size):\n",
    "                this_data, this_sample_info = dataset[np.random.randint(0, len(dataset))]\n",
    "                batch_data.append(this_data)\n",
    "                this_sample_info['class_id'] = dataset.dataset.class_name_to_class_id(this_sample_info['name'])\n",
    "                all_sample_info.append(this_sample_info)\n",
    "            batch_data = torch.stack(batch_data).half().cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_latents = model.encode(batch_data).cpu().numpy()\n",
    "            all_latents.append(batch_latents)\n",
    "        \n",
    "        all_latents = np.concatenate(all_latents, axis=0)\n",
    "        all_ids = np.array([x['class_id'] for x in all_sample_info])\n",
    "        all_text_features = np.array([x['text_feature'] for x in all_sample_info])\n",
    "\n",
    "        features[model_config][dataset.dataset.dataset_name] = {\n",
    "            'latents': all_latents,\n",
    "            'ids': all_ids,\n",
    "            'text_features': all_text_features,\n",
    "        }\n",
    "        np.save('/home/dchenbs/workspace/cache/temp/features.npy', features)\n",
    "features = np.load('/home/dchenbs/workspace/cache/temp/features.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def get_knn_similarity(latents, references, k=10):\n",
    "    if None in references:\n",
    "        return 0\n",
    "    knn = NearestNeighbors(n_neighbors=k)\n",
    "    knn.fit(latents)\n",
    "    overall_similarity = 0\n",
    "    \n",
    "    for i in tqdm.tqdm(range(len(latents))):\n",
    "        latent = latents[i]\n",
    "        distances, indices = knn.kneighbors([latent])\n",
    "        positive_references = references[indices[0]]\n",
    "        centroids = positive_references[0].reshape(1, -1)\n",
    "        similarities = cosine_similarity(centroids, positive_references)\n",
    "        overall_similarity += np.mean(similarities)\n",
    "    \n",
    "    return overall_similarity/len(latents)\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "def tsne_visualize(latents, ids, title=''):\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    latents_tsne = tsne.fit_transform(latents)\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(latents_tsne[:, 0], latents_tsne[:, 1], c=ids, cmap='tab20', s=10, alpha=0.5)\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    # plt.savefig(os.path.join(vis_dir, f'tsne-{dataset.dataset.dataset_name}-{model_dir.split(\"/\")[-1]}.png'), bbox_inches='tight', pad_inches=0)\n",
    "    plt.show()\n",
    "\n",
    "import umap\n",
    "def umap_visualize(latents, ids, title=''):\n",
    "    latents_umap = umap.UMAP().fit_transform(latents)\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(latents_umap[:, 0], latents_umap[:, 1], c=ids, cmap='tab20', s=10, alpha=0.5)\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    # plt.savefig(os.path.join(vis_dir, f'tsne-{dataset.dataset.dataset_name}-{model_dir.split(\"/\")[-1]}.png'), bbox_inches='tight', pad_inches=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def linear_classification_eevaluation(latents, ids, val_split_ratio=0.5):\n",
    "\n",
    "    train_latents, val_latents, train_ids, val_ids = train_test_split(latents, ids, test_size=val_split_ratio, random_state=42)\n",
    "    clf = LogisticRegression(random_state=42, max_iter=2000).fit(train_latents, train_ids)\n",
    "    predicted_ids = clf.predict(val_latents)\n",
    "    return accuracy_score(val_ids, predicted_ids)\n",
    "\n",
    "\n",
    "def knn_classification_evaluation(latents, ids, val_split_ratio=0.5, k=5):\n",
    "    \n",
    "    train_latents, val_latents, train_ids, val_ids = train_test_split(latents, ids, test_size=val_split_ratio, random_state=42)\n",
    "    knn = NearestNeighbors(n_neighbors=k)\n",
    "    knn.fit(train_latents)\n",
    "\n",
    "    overall_similarity = 0\n",
    "    for i in tqdm.tqdm(range(len(val_latents))):\n",
    "        latent = val_latents[i]\n",
    "        distances, indices = knn.kneighbors([latent])\n",
    "        class_ids = train_ids[indices[0]]\n",
    "        class_id = np.argmax(np.bincount(class_ids))\n",
    "        overall_similarity += (class_id == val_ids[i])\n",
    "    return overall_similarity/len(val_latents)\n",
    "\n",
    "dataset_name = 'coco'\n",
    "for model_config in model_configs:\n",
    "    print(f'{model_config}')\n",
    "    features_dict = features[model_config][dataset_name]\n",
    "    predictions = features_dict['latents']\n",
    "    reference = features_dict['text_features']\n",
    "    ids = features_dict['ids']\n",
    "\n",
    "    # tsne_visualize(predictions, ids, title=f'{model_config.split(\"/\")[-2]}-{dataset_name}')\n",
    "    # umap_visualize(predictions, ids, title=f'{model_config.split(\"/\")[-2]}-{dataset_name}')\n",
    "\n",
    "    # acc = linear_classification_eevaluation(predictions, ids, train_val_split_ratio=0.5)\n",
    "    # print(f'Linear classification accuracy: {acc}')\n",
    "    acc = knn_classification_evaluation(predictions, ids, val_split_ratio=0.2, k=8)\n",
    "    print(f'KNN classification accuracy: {acc}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq2seq-ae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
