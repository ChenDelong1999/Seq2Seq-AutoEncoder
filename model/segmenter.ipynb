{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(0)\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'\n",
    "from scipy.ndimage import label\n",
    "import time\n",
    "import torch\n",
    "import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from segmenter import Segmenter\n",
    "\n",
    "def visualized_masks(masks, image):\n",
    "    canvas = np.ones_like(image) * 255\n",
    "    masks = sorted(masks, key=lambda x: x['area'], reverse=True)\n",
    "    for mask in masks:\n",
    "        average_color = np.mean(image[mask['segmentation'] == 1], axis=0)\n",
    "        canvas[mask['segmentation'] == 1] = average_color\n",
    "\n",
    "        # visualize segment boundary\n",
    "        contours, _ = cv2.findContours(mask['segmentation'].astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cv2.drawContours(canvas, contours, -1, (200, 200, 200), 1)\n",
    "\n",
    "    return canvas\n",
    "\n",
    "\n",
    "def get_masked_area(masks):\n",
    "    masked_area = None\n",
    "    for mask in masks:\n",
    "        if masked_area is None:\n",
    "            masked_area = mask['segmentation'].astype(np.uint8)\n",
    "        else:\n",
    "            masked_area[mask['segmentation']] += 1\n",
    "\n",
    "    non_masked_area = (masked_area == 0).astype(np.uint8)\n",
    "    return masked_area, non_masked_area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(0)\n",
    "import os\n",
    "from scipy.ndimage import label\n",
    "import time\n",
    "import torch\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class Segmenter():\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            model_name,\n",
    "            checkpoint,\n",
    "            points_per_side = 32,\n",
    "            points_per_batch = 64,\n",
    "            pred_iou_thresh = 0.88,\n",
    "            stability_score_thresh = 0.95,\n",
    "            stability_score_offset = 1.0,\n",
    "            box_nms_thresh = 0.7,\n",
    "            crop_n_layers = 0,\n",
    "            crop_nms_thresh = 0.7,\n",
    "            crop_overlap_ratio = 512 / 1500,\n",
    "            crop_n_points_downscale_factor = 1,\n",
    "            min_mask_region_area = 0,\n",
    "            device = 'cuda',\n",
    "            ):\n",
    "        self.generator = None\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        if self.model_name=='fast_sam':\n",
    "            # Fast Segment Anything \n",
    "            # https://arxiv.org/abs/2306.12156 (21 Jun 2023)\n",
    "            # https://github.com/CASIA-IVA-Lab/FastSAM\n",
    "\n",
    "            from fastsam import FastSAM\n",
    "            self.generator = FastSAM(checkpoint)\n",
    "\n",
    "        elif self.model_name=='mobile_sam':\n",
    "            # Faster Segment Anything: Towards Lightweight SAM for Mobile Applications \n",
    "            # https://arxiv.org/abs/2306.14289.pdf (25 Jun 2023)\n",
    "            # https://github.com/ChaoningZhang/MobileSAM\n",
    "            \n",
    "            from mobile_sam import sam_model_registry, SamAutomaticMaskGenerator\n",
    "            sam = sam_model_registry[\"vit_t\"](checkpoint=checkpoint).to(device).eval()\n",
    "\n",
    "        elif self.model_name=='repvit_sam':\n",
    "            from repvit_sam import SamAutomaticMaskGenerator, SamPredictor, sam_model_registry\n",
    "            sam = sam_model_registry[\"repvit\"](checkpoint=checkpoint).to(device).eval()\n",
    "\n",
    "        elif self.model_name=='sam':\n",
    "            from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
    "            model_type = checkpoint.split('/')[-1][4:9]\n",
    "            sam = sam_model_registry[model_type](checkpoint=checkpoint).to(device).eval()\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(f'Model {self.model_name} not implemented')\n",
    "        \n",
    "        if self.generator is None:\n",
    "            self.generator = SamAutomaticMaskGenerator(\n",
    "                sam,\n",
    "                points_per_side=points_per_side,\n",
    "                points_per_batch=points_per_batch,\n",
    "                pred_iou_thresh=pred_iou_thresh,\n",
    "                stability_score_thresh=stability_score_thresh,\n",
    "                stability_score_offset=stability_score_offset,\n",
    "                box_nms_thresh=box_nms_thresh,\n",
    "                crop_n_layers=crop_n_layers,\n",
    "                crop_nms_thresh=crop_nms_thresh,\n",
    "                crop_overlap_ratio=crop_overlap_ratio,\n",
    "                crop_n_points_downscale_factor=crop_n_points_downscale_factor,\n",
    "                min_mask_region_area=min_mask_region_area,\n",
    "                )\n",
    "        \n",
    "        \n",
    "    def __call__(self, image_path, post_processing=True):  \n",
    "        \n",
    "        image = np.array(Image.open(image_path).convert('RGB')) \n",
    "        \n",
    "        if self.model_name=='fast_sam':\n",
    "            everything_results = self.generator(image_path, device='cuda', retina_masks=True, imgsz=1024, conf=0.4, iou=0.9,)\n",
    "            masks = []\n",
    "            for i in range(everything_results[0].boxes.data.shape[0]):\n",
    "                box = everything_results[0].boxes.data[i]\n",
    "                mask = everything_results[0].masks.data[i]\n",
    "                masks.append({'segmentation': mask.cpu().numpy().astype(bool), 'area': mask.sum(), 'bbox': box.cpu().tolist(),})\n",
    "        else:\n",
    "            masks = self.generator.generate(image)\n",
    "\n",
    "        if post_processing:\n",
    "            masks = self.post_processing_masks(masks, image)\n",
    "        return masks\n",
    "\n",
    "    def expand_mask_blur(self, mask, kernel):\n",
    "        mask = mask.copy()\n",
    "        mask['segmentation'] = mask['segmentation'].astype(np.uint8)\n",
    "        blurred_mask = cv2.filter2D(mask['segmentation'],-1,kernel)\n",
    "        expanded_mask = (blurred_mask > 0).astype(bool)\n",
    "        return expanded_mask\n",
    "\n",
    "\n",
    "    def post_processing_masks(self, masks, image):\n",
    "\n",
    "        kernel_size = int(min(image.shape[:2]) * 0.015) // 2 * 2 + 1\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(kernel_size,kernel_size))\n",
    "\n",
    "        masked_area = None\n",
    "        post_processed_masks = []\n",
    "        for mask in masks:\n",
    "            expanded_mask = self.expand_mask_blur(mask, kernel)\n",
    "            post_processed_masks.append({\n",
    "                'segmentation': expanded_mask.astype(bool),\n",
    "                'area': expanded_mask.sum(),\n",
    "                'bbox': list(cv2.boundingRect(expanded_mask.astype(np.uint8))),\n",
    "            })\n",
    "            if masked_area is None:\n",
    "                masked_area = expanded_mask.astype(np.uint8)\n",
    "            else:\n",
    "                masked_area[expanded_mask] += 1\n",
    "\n",
    "        non_masked_area = masked_area == 0\n",
    "        labeled_mask, num_labels = label(non_masked_area)\n",
    "        \n",
    "        for i in range(1, num_labels + 1):\n",
    "            post_processed_masks.append({\n",
    "                'segmentation': labeled_mask == i,\n",
    "                'area': (labeled_mask == i).sum(),\n",
    "                'bbox': list(cv2.boundingRect((labeled_mask == i).astype(np.uint8))),\n",
    "            })\n",
    "        return post_processed_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running [MOBILE_SAM]: mobile_sam.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:11<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MOBILE_SAM]: mobile_sam.pt\n",
      "1.149364s/image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_test_images = 10\n",
    "visualize = False\n",
    "\n",
    "img_dir = '/home/dchenbs/workspace/datasets/coco2017/images/val2017'\n",
    "image_paths = []\n",
    "for i in range(num_test_images):\n",
    "    img_path = os.path.join(img_dir, random.choice(os.listdir(img_dir)))\n",
    "    image_paths.append(img_path)\n",
    "\n",
    "\n",
    "models = [\n",
    "    \n",
    "    # ('sam', '/home/dchenbs/workspace/cache/sam_vit_h_4b8939.pth'), \n",
    "        # 6.798 GB, 2.47 s/image\n",
    "\n",
    "    # ('sam', '/home/dchenbs/workspace/cache/sam_vit_l_0b3195.pth'), \n",
    "        # 5.346 GB, 1.76 s/image\n",
    "\n",
    "    # ('sam', '/home/dchenbs/workspace/cache/sam_vit_b_01ec64.pth'), \n",
    "        # 4.404 GB, 1.14 s/image\n",
    "\n",
    "    # ('fast_sam', '/home/dchenbs/workspace/cache/FastSAM-s.pt'),  \n",
    "        # 1.326 GB, 0.34 s/image\n",
    "\n",
    "    # ('fast_sam', '/home/dchenbs/workspace/cache/FastSAM-x.pt'), \n",
    "        # 1.946 GB, 0.24 s/image\n",
    "    \n",
    "    ('mobile_sam', '/home/dchenbs/workspace/cache/mobile_sam.pt'),\n",
    "        # 4.376 GB, 1.15 s/image\n",
    "    \n",
    "    # ('repvit_sam', '/home/dchenbs/workspace/Seq2Seq-AutoEncoder/RepViT/sam/weights/repvit_sam.pt'), \n",
    "        # 4.722 GB, 1.49 s/image\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "for model_name, checkpoint in models:\n",
    "    start = time.time()\n",
    "    print(f'Running [{model_name.upper()}]: {checkpoint.split(\"/\")[-1]}')\n",
    "\n",
    "    segmenter = None\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    segmenter = Segmenter(model_name, checkpoint)\n",
    "    for img_path in tqdm.tqdm(image_paths):\n",
    "        masks = segmenter(img_path, post_processing=True)\n",
    "        image = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        if visualize:\n",
    "            plt.figure(figsize=(20, 8))\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(image)\n",
    "            plt.axis('off')\n",
    "\n",
    "            canvas = visualized_masks(masks, image)\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(canvas)\n",
    "            plt.axis('off')\n",
    "\n",
    "            masked_area, non_masked_area = get_masked_area(masks)\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.imshow(masked_area*int(255/max(masked_area.flatten())))\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    print(f'[{model_name.upper()}]: {checkpoint.split(\"/\")[-1]}\\n{(time.time()-start)/num_test_images :2f}s/image')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq2seq-ae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
