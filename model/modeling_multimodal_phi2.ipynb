{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dchenbs/anaconda3/envs/seq2seq-ae/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(0)\n",
    "from PIL import Image\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# !huggingface-cli login --token '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.41it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from phi import PhiForCausalLM, PhiConfig\n",
    "from phi.modeling_phi import *\n",
    "from seq2seq_autoencoder import Seq2SeqAutoEncoderModel\n",
    "from transformers.utils import ModelOutput\n",
    "\n",
    "\n",
    "class PhiModel(PhiPreTrainedModel):\n",
    "    \"\"\"Phi model.\"\"\"\n",
    "\n",
    "    _keys_to_ignore_on_load_missing = [\"\"]\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"h\\.\\d+\\.mlp.(fc_in|fc_out)\\.(weight|bias)\"]\n",
    "\n",
    "    def __init__(self, config: PhiConfig) -> None:\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.embd = Embedding(config)\n",
    "        self.h = nn.ModuleList([ParallelBlock(config, block_idx=i) for i in range(config.n_layer)])\n",
    "        self.gradient_checkpointing = False\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self) -> nn.Embedding:\n",
    "        return self.embd.wte\n",
    "\n",
    "    def set_input_embeddings(self, new_embeddings: nn.Embedding) -> None:\n",
    "        self.embd.wte = new_embeddings\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        segment_tokens: Optional[torch.FloatTensor] = None,\n",
    "        segment_position_ids: Optional[list] = None,\n",
    "        past_key_values: Optional[Union[torch.FloatTensor, InferenceParams]] = None,\n",
    "        attention_mask: Optional[torch.BoolTensor] = None,\n",
    "    ) -> torch.FloatTensor:\n",
    "        hidden_states = self.embd(input_ids)\n",
    "\n",
    "        if segment_tokens is not None:\n",
    "            assert segment_position_ids is not None\n",
    "            for i in range(len(segment_tokens)):\n",
    "                if len(segment_tokens[i]) != 0:\n",
    "                    hidden_states[i, segment_position_ids[i]] += segment_tokens[i]\n",
    "\n",
    "        for layer in self.h:\n",
    "            hidden_states = layer(\n",
    "                hidden_states,\n",
    "                past_key_values=past_key_values,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class SegmentHead(nn.Module):\n",
    "\n",
    "    def __init__(self, d_llm, d_segment_latent) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln = nn.LayerNorm(d_llm)\n",
    "        self.linear_to_latent = nn.Linear(d_llm, d_segment_latent)\n",
    "        self.linear_to_bbox = nn.Linear(d_llm, 4)\n",
    "\n",
    "    def forward(self, hidden_states: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        hidden_states = self.ln(hidden_states)\n",
    "        latents = self.linear_to_latent(hidden_states).to(torch.float32)\n",
    "        bboxes = self.linear_to_bbox(hidden_states).to(torch.float32)\n",
    "\n",
    "        return latents, bboxes\n",
    "\n",
    "\n",
    "class RegreessionLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, shift_labels: bool = True) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.shift_labels = shift_labels\n",
    "        self.loss_fct = nn.MSELoss()\n",
    "\n",
    "    def forward(self, prediction: torch.FloatTensor, target: torch.LongTensor) -> torch.FloatTensor:\n",
    "        if self.shift_labels:\n",
    "            prediction = prediction[..., :-1, :].contiguous()\n",
    "            target = target[..., 1:].contiguous()\n",
    "\n",
    "        loss = self.loss_fct(prediction, target)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MultimodalCausalLMOutputWithPast(ModelOutput):\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    predicted_segment_latents: Optional[torch.FloatTensor] = None\n",
    "    predicted_segment_bboxes: Optional[torch.FloatTensor] = None\n",
    "    \n",
    "\n",
    "class PhiForMultimodal(PhiForCausalLM):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            config: PhiConfig,\n",
    "            w_segment_loss: float = 1.0,\n",
    "            w_bbox_loss: float = 1.0,\n",
    "            ) -> None:\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.transformer = PhiModel(config)\n",
    "        self.lm_head = CausalLMHead(config)\n",
    "        self.lm_loss = CausalLMLoss()\n",
    "        self.segment_loss = RegreessionLoss(shift_labels=False)\n",
    "        self.seqae_loaded = False\n",
    "\n",
    "        self.w_segment_loss=w_segment_loss\n",
    "        self.w_bbox_loss=w_bbox_loss\n",
    "\n",
    "        self.post_init()\n",
    "\n",
    "    def load_seqae(self, seqae_path: str, freeze_seqae_encoder: bool = False, freeze_seqae_decoder: bool = False):\n",
    "        self.seqae = Seq2SeqAutoEncoderModel.from_pretrained(seqae_path).to(self.device)\n",
    "        if freeze_seqae_encoder:\n",
    "            for param in self.seqae.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        if freeze_seqae_decoder:\n",
    "            for param in self.seqae.decoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.visual_token_embedding = torch.nn.Linear(self.seqae.config.d_latent+4, self.config.n_embd, bias=False).to(self.device)\n",
    "        self.segment_head = SegmentHead(self.config.n_embd, self.seqae.config.d_latent).to(self.device)\n",
    "        self.seqae_loaded = True\n",
    "\n",
    "    def preprocess_segments(self, segment_masks, images):\n",
    "\n",
    "        def resize(segment):\n",
    "            # segment['patch'] is PIL Image object\n",
    "            w, h = segment['patch'].size\n",
    "            if h * w > (self.seqae.config.data_seq_length):\n",
    "                ratio_to_maxlength = np.sqrt(self.seqae.config.data_seq_length / (h * w))\n",
    "                h = int(h * ratio_to_maxlength)\n",
    "                w = int(w * ratio_to_maxlength)\n",
    "                segment['patch'] = torchvision.transforms.Resize([h, w])(segment['patch'])\n",
    "                segment['mask'] = torchvision.transforms.Resize([h, w])(segment['mask'][None, :, :])[0]\n",
    "\n",
    "            return segment\n",
    "        \n",
    "        def encode_to_sequence(segment):\n",
    "            # segment['patch'] is torch tensor with shape (C, H, W)\n",
    "            h, w = segment['patch'].shape[1:]\n",
    "            sequence = []\n",
    "            for i in range(h):\n",
    "                for j in range(w):\n",
    "                    pixel_data = segment['patch'][:, i, j] / 255.0\n",
    "                    is_rightmost = 1 if j == w - 1 else 0\n",
    "                    is_non_masked = int(segment['mask'][i, j])\n",
    "                    sequence.append(pixel_data.tolist() + [is_rightmost, is_non_masked])\n",
    "            sequence = np.array(sequence) \n",
    "\n",
    "            # pad the sequence to max_seq_length with zeros\n",
    "            if len(sequence) < self.seqae.config.data_seq_length:\n",
    "                sequence = np.concatenate((sequence, np.zeros((self.seqae.config.data_seq_length - len(sequence), self.seqae.config.input_channels))))\n",
    "\n",
    "            # add the query place holder to the end of the sequence\n",
    "            sequence = np.concatenate((sequence, np.zeros((self.seqae.config.num_queries, self.seqae.config.input_channels))))\n",
    "            # add one all zero column to the start \n",
    "            sequence = np.concatenate((np.zeros((1, sequence.shape[1])), sequence), axis=0)\n",
    "\n",
    "            return torch.from_numpy(sequence)\n",
    "    \n",
    "        segment_sequences = []\n",
    "        bboxes = []\n",
    "        for segment_mask in segment_masks:\n",
    "            image = images[segment_mask[\"image_index\"]]\n",
    "\n",
    "            mask = segment_mask[\"mask\"]\n",
    "            bbox = segment_mask[\"bbox\"]\n",
    "            bbox[2] = 1 if bbox[2] == 0 else bbox[2]\n",
    "            bbox[3] = 1 if bbox[3] == 0 else bbox[3]\n",
    "            x, y, w, h = bbox\n",
    "\n",
    "            segment = {\n",
    "                \"patch\": image.crop((x, y, x + w, y + h)),\n",
    "                \"mask\": mask[y:y+h, x:x+w],\n",
    "            }\n",
    "\n",
    "            segment = resize(segment)\n",
    "            segment['patch'] = torchvision.transforms.ToTensor()(segment['patch'])\n",
    "            segment['patch'] = segment['patch'] * segment['mask'][None, :, :]\n",
    "            segment_sequence = encode_to_sequence(segment)\n",
    "            segment_sequence = segment_sequence.to(self.device)\n",
    "            segment_sequences.append(segment_sequence)\n",
    "            bboxes.append(bbox)\n",
    "\n",
    "        segment_sequences = torch.stack(segment_sequences, dim=0).to(self.device, dtype=self.dtype)\n",
    "        bboxes = torch.tensor(bboxes).to(self.device, dtype=self.dtype)\n",
    "        return segment_sequences, bboxes\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        segment_masks: Optional[list] = None,\n",
    "            # A list of batch_size number of list of dicts, whose key includes \n",
    "            #   - \"image_index\": int\n",
    "            #   - \"mask\": binary mask of torch.Tensor with shape (H, W)\n",
    "            #   - \"bbox\": bounding box of the object in the image (x, y, w, h)\n",
    "            # if no segment token in certain sample, the corresponding list should be empty\n",
    "        images: Optional[list] = None,\n",
    "            # A list of PIL Images objects\n",
    "        seqae_batch_size: Optional[int] = 16,\n",
    "            # batch size for seqae inference, -1 for full segments inference for each sample\n",
    "        past_key_values: Optional[Union[torch.FloatTensor, InferenceParams]] = None,\n",
    "        attention_mask: Optional[torch.BoolTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> CausalLMOutputWithPast:\n",
    "        \n",
    "        if not self.seqae_loaded:\n",
    "            raise ValueError(\"Please load seqae model first.\")\n",
    "        \n",
    "        if segment_masks is not None:\n",
    "            batch_segment_tokens = []\n",
    "            batch_segment_bboxes = []\n",
    "            batch_segment_latents = []\n",
    "            for sample_segment_masks in segment_masks:\n",
    "                if len(sample_segment_masks)!=0:\n",
    "                    segment_sequences, bboxes = self.preprocess_segments(sample_segment_masks, images)\n",
    "\n",
    "                    if seqae_batch_size != -1:\n",
    "                        segment_latents = []\n",
    "                        for i in range(0, len(segment_sequences), seqae_batch_size):\n",
    "                            segment_latents.append(self.seqae.encode(segment_sequences[i:i+seqae_batch_size]))\n",
    "                        segment_latents = torch.cat(segment_latents, dim=0)\n",
    "                    else:\n",
    "                        segment_latents = self.seqae.encode(segment_sequences)\n",
    "\n",
    "                    latents_and_bboxes = torch.cat((segment_latents, bboxes), dim=1)\n",
    "                    segment_tokens = self.visual_token_embedding(latents_and_bboxes)\n",
    "                    batch_segment_tokens.append(segment_tokens)\n",
    "                    batch_segment_bboxes.append(bboxes)\n",
    "                    batch_segment_latents.append(segment_latents)\n",
    "                else:\n",
    "                    batch_segment_tokens.append([])\n",
    "                    batch_segment_bboxes.append([])\n",
    "                    batch_segment_latents.append([])\n",
    "                \n",
    "            # find the position of <|seg|> in input_ids\n",
    "            batch_segment_position_ids = []\n",
    "            for i in range(len(input_ids)):\n",
    "                seg_pos = torch.where(input_ids[i] == self.special_token_id_mappinmg[\"<|seg|>\"])[0].tolist()\n",
    "                assert len(seg_pos) == len(segment_masks[i]), f\"number of <|seg|> in input_ids ({len(seg_pos)}) does not match number of segments ({len(segment_masks[i])})\"\n",
    "                batch_segment_position_ids.append(seg_pos)\n",
    "        else:\n",
    "            batch_segment_tokens = None\n",
    "            batch_segment_bboxes = None\n",
    "            batch_segment_latents = None\n",
    "            batch_segment_position_ids = None\n",
    "            \n",
    "        hidden_states = self.transformer(\n",
    "            input_ids, \n",
    "            segment_tokens=batch_segment_tokens,\n",
    "            segment_position_ids=batch_segment_position_ids,\n",
    "            past_key_values=past_key_values, \n",
    "            attention_mask=attention_mask\n",
    "            )\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "\n",
    "        # LLM Transformer last hidden state predicts next text token\n",
    "        loss = None\n",
    "        lm_loss = None\n",
    "        if labels is not None:\n",
    "            lm_loss = self.lm_loss(lm_logits, labels)\n",
    "\n",
    "        # LLM Transformer last hidden state predicts segment latent and bbox\n",
    "        segment_loss = 0\n",
    "        bbox_loss = 0\n",
    "        if segment_masks is not None:\n",
    "            predicted_segment_latents = []\n",
    "            predicted_segment_bboxes = []\n",
    "            for i in range(len(batch_segment_tokens)):\n",
    "                if len(batch_segment_tokens[i]) != 0:\n",
    "                    segment_latents, segment_bboxes = self.segment_head(hidden_states[i, batch_segment_position_ids[i]])\n",
    "                    segment_loss += self.segment_loss(segment_latents, batch_segment_latents[i])\n",
    "                    bbox_loss += self.segment_loss(segment_bboxes, batch_segment_bboxes[i])\n",
    "\n",
    "                    predicted_segment_latents.append(segment_latents)\n",
    "                    predicted_segment_bboxes.append(segment_bboxes)\n",
    "                else:\n",
    "                    predicted_segment_latents.append([])\n",
    "                    predicted_segment_bboxes.append([])\n",
    "        else:\n",
    "            predicted_segment_latents = None\n",
    "            predicted_segment_bboxes = None\n",
    "\n",
    "        loss = lm_loss + self.w_segment_loss * segment_loss + self.w_bbox_loss * bbox_loss\n",
    "\n",
    "        return MultimodalCausalLMOutputWithPast(\n",
    "            loss=loss, \n",
    "            logits=lm_logits, \n",
    "            past_key_values=past_key_values,\n",
    "            hidden_states=hidden_states,\n",
    "            predicted_segment_latents=predicted_segment_latents,\n",
    "            predicted_segment_bboxes=predicted_segment_bboxes,\n",
    "            )\n",
    "\n",
    "\n",
    "model = PhiForMultimodal.from_pretrained(\n",
    "    \"microsoft/phi-2\",\n",
    "    w_segment_loss=1.0,\n",
    "    w_bbox_loss=1.0,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "model.load_seqae('/home/dchenbs/workspace/Seq2Seq-AutoEncoder/runs/Nov28_20-50-04_host19-SA1B-[327MB-16queries-1024]-[lr1e-05-bs16x1step-8gpu]/checkpoints/checkpoint_ep2_step3200k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
    "tokenizer.add_tokens([\"<|startofimage|>\", \"<|endofimage|>\", \"<|seg|>\"])\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.special_token_id_mappinmg = {\n",
    "    \"<|startofimage|>\": tokenizer.convert_tokens_to_ids(\"<|startofimage|>\"),\n",
    "    \"<|endofimage|>\": tokenizer.convert_tokens_to_ids(\"<|endofimage|>\"),\n",
    "    \"<|seg|>\": tokenizer.convert_tokens_to_ids(\"<|seg|>\"),\n",
    "    \"<|endoftext|>\": tokenizer.convert_tokens_to_ids(\"<|endoftext|>\"),\n",
    "    \"[PAD]\": tokenizer.convert_tokens_to_ids(\"[PAD]\"),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SAM model vit_b from /home/dchenbs/workspace/cache/sam_vit_b_01ec64.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from segmenter import Segmenter\n",
    "checkpoint = \"/home/dchenbs/workspace/cache/sam_vit_b_01ec64.pth\"\n",
    "segmenter = Segmenter(checkpoint, device=DEVICE)\n",
    "\n",
    "def segment_one_image(image_path, visualize=False):\n",
    "\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_np = np.array(image)\n",
    "    segment_masks = segmenter(image_np)\n",
    "\n",
    "    def visualized_masks(masks, image):\n",
    "        canvas = np.ones_like(image) * 255\n",
    "        masks = sorted(masks, key=lambda x: x['area'], reverse=True)\n",
    "        for mask in masks:\n",
    "            average_color = np.mean(image[mask['mask'] == 1], axis=0)\n",
    "            canvas[mask['mask'] == 1] = average_color\n",
    "\n",
    "            # visualize segment boundary\n",
    "            contours, _ = cv2.findContours(mask['mask'].astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.drawContours(canvas, contours, -1, (200, 200, 200), 1)\n",
    "        return canvas\n",
    "    \n",
    "    if visualize:\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(image_np)\n",
    "        plt.axis('off')\n",
    "\n",
    "        canvas = visualized_masks(segment_masks, image_np)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(canvas)\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    for segment_mask in segment_masks:\n",
    "        segment_mask['image_index'] = 0\n",
    "        segment_mask['mask'] = torch.from_numpy(segment_mask['mask'])\n",
    "        \n",
    "    return segment_masks, [image]\n",
    "\n",
    "img_dir = '/home/dchenbs/workspace/datasets/coco2017/images/val2017'\n",
    "img_path = os.path.join(img_dir, random.choice(os.listdir(img_dir)))\n",
    "segment_masks, images = segment_one_image(img_path, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hellow world![PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "This is a segment token: <|startofimage|><|seg|><|endofimage|><|endoftext|>[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "This is an image of a cat with 4 tokens: <|startofimage|><|seg|><|seg|><|seg|><|seg|><|endofimage|>!<|endoftext|>\n",
      "{'input_ids': tensor([[   39,  5037,   995,     0, 50298, 50298, 50298, 50298, 50298, 50298,\n",
      "         50298, 50298, 50298, 50298, 50298, 50298, 50298, 50298, 50298, 50298],\n",
      "        [ 1212,   318,   257, 10618, 11241,    25,   220, 50295, 50297, 50296,\n",
      "         50256, 50298, 50298, 50298, 50298, 50298, 50298, 50298, 50298, 50298],\n",
      "        [ 1212,   318,   281,  2939,   286,   257,  3797,   351,   604, 16326,\n",
      "            25,   220, 50295, 50297, 50297, 50297, 50297, 50296,     0, 50256]],\n",
      "       device='cuda:0')} torch.Size([3, 20])\n",
      "{'<|startofimage|>': 50295, '<|endofimage|>': 50296, '<|seg|>': 50297, '<|endoftext|>': 50256, '[PAD]': 50298}\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dchenbs/anaconda3/envs/seq2seq-ae/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultimodalCausalLMOutputWithPast(loss=tensor(20469.7910, device='cuda:0', grad_fn=<AddBackward0>), logits=tensor([[[ 5.2736,  7.9187,  4.2404,  ..., -2.9310, -2.9312, -2.9305],\n",
      "         [10.1601, 10.7517,  3.4917,  ..., -2.3451, -2.3445, -2.3443],\n",
      "         [17.2196, 14.6495,  8.3551,  ..., -0.6489, -0.6487, -0.6492],\n",
      "         ...,\n",
      "         [10.8129, 12.5818,  9.4468,  ..., -0.3477, -0.3487, -0.3476],\n",
      "         [10.9866, 12.8867,  9.6272,  ..., -0.2689, -0.2699, -0.2688],\n",
      "         [12.3153, 13.7971, 11.0963,  ..., -0.0814, -0.0826, -0.0816]],\n",
      "\n",
      "        [[ 6.4830,  6.1644,  3.4055,  ..., -4.0035, -4.0029, -4.0029],\n",
      "         [ 6.0642,  7.8241,  3.4634,  ..., -2.5955, -2.5946, -2.5961],\n",
      "         [ 3.9298,  7.1021,  1.9519,  ..., -1.8224, -1.8218, -1.8224],\n",
      "         ...,\n",
      "         [ 8.6108, 12.4696,  6.3324,  ...,  0.8407,  0.8396,  0.8404],\n",
      "         [ 8.0109, 12.4024,  6.1988,  ...,  0.7150,  0.7140,  0.7150],\n",
      "         [ 9.1022, 13.4377,  7.7072,  ...,  0.5867,  0.5861,  0.5863]],\n",
      "\n",
      "        [[ 6.4830,  6.1644,  3.4055,  ..., -4.0035, -4.0029, -4.0029],\n",
      "         [ 6.0642,  7.8241,  3.4634,  ..., -2.5955, -2.5946, -2.5961],\n",
      "         [ 5.3335,  7.2349,  2.0610,  ..., -1.1670, -1.1662, -1.1666],\n",
      "         ...,\n",
      "         [ 7.6920, 11.5031,  7.9288,  ..., -1.3221, -1.3232, -1.3227],\n",
      "         [ 6.2080,  4.0803,  8.9128,  ..., -2.1850, -2.1862, -2.1860],\n",
      "         [ 7.4105,  9.9500, 12.1976,  ..., -3.6484, -3.6488, -3.6478]]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>), past_key_values=None, hidden_states=tensor([[[  1.4970,   2.0012,  -3.6550,  ...,  -0.3971,  -1.9681,  -0.9979],\n",
      "         [  1.1524,  -0.7598,   4.3587,  ...,  -1.9878,   0.4580,  -1.9609],\n",
      "         [  1.4814,   1.0530,   3.3373,  ...,   4.1383,   0.3930,   0.5255],\n",
      "         ...,\n",
      "         [  0.6186,  -1.2267,   1.9873,  ...,  -5.6329,  -4.4461,  -2.3477],\n",
      "         [  0.7796,  -1.4083,   2.1578,  ...,  -5.6102,  -3.7992,  -1.7356],\n",
      "         [  0.2553,  -1.2429,   1.0572,  ...,  -4.2611,  -3.2240,  -1.1927]],\n",
      "\n",
      "        [[  2.1361,   2.7436,  -2.7581,  ...,   0.7276,  -0.3326,   0.3185],\n",
      "         [  3.8379,   4.5650,   0.3494,  ...,   4.8752,  -4.7400,  -0.1798],\n",
      "         [  1.7792,   1.0935,  -0.7866,  ...,  -3.3505,   0.5211,   0.0623],\n",
      "         ...,\n",
      "         [ -1.5482,  -3.1475,  -3.1831,  ...,   1.1290,  -0.9378,   0.3114],\n",
      "         [ -1.1881,  -2.3842,  -2.2671,  ...,   0.6046,  -0.3713,   0.1647],\n",
      "         [ -0.1892,  -1.6187,  -1.9675,  ...,   3.8065,  -0.6961,   0.8351]],\n",
      "\n",
      "        [[  2.1361,   2.7436,  -2.7581,  ...,   0.7276,  -0.3326,   0.3185],\n",
      "         [  3.8379,   4.5650,   0.3494,  ...,   4.8752,  -4.7400,  -0.1798],\n",
      "         [ -0.7626,   0.4452,  -5.3642,  ...,  -2.1035,  -0.3577,   1.4404],\n",
      "         ...,\n",
      "         [  2.1969,  -0.9948,  -1.0349,  ...,  -3.1859,  -2.4895,   0.8575],\n",
      "         [  2.1360,   0.3967,  -1.4075,  ...,  -4.3005,  -1.6687,  -2.0467],\n",
      "         [  0.1589,  -0.5752, -10.2416,  ...,  -3.7652,   0.5567,   4.1979]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>), attentions=None, predicted_segment_latents=[[], tensor([[-1.6434e-01, -8.6617e-01, -3.6415e-01, -2.5022e-01, -5.0165e-01,\n",
      "          6.5554e-01, -4.8500e-01,  5.0687e-02,  4.6656e-02,  6.8293e-01,\n",
      "          3.8264e-01, -5.9800e-01, -5.8082e-01,  1.0327e-01,  8.4202e-01,\n",
      "          3.4969e-01, -8.6849e-01,  8.0704e-01, -7.2670e-01,  1.2379e+00,\n",
      "          1.6747e-01, -2.6007e-01, -7.6277e-01, -2.2591e-01, -1.5088e-01,\n",
      "         -6.9663e-01, -3.4823e-01,  9.8090e-02,  2.0846e-01,  9.7393e-01,\n",
      "          4.9249e-01,  1.5324e+00,  1.3884e+00, -5.7269e-01,  2.7227e-01,\n",
      "         -2.1613e-01, -4.2149e-01,  2.5870e-01, -1.3197e-01,  1.2123e-01,\n",
      "         -9.6925e-01, -4.4127e-01,  4.3187e-01,  1.0368e+00,  6.2740e-01,\n",
      "         -4.0619e-01, -7.6265e-01, -4.7312e-01, -4.9744e-02, -5.4784e-01,\n",
      "         -1.5248e-01, -5.2039e-01,  6.3666e-01,  2.8652e-01,  8.9118e-02,\n",
      "          7.8979e-02, -1.0559e-01, -7.8482e-01, -1.2650e-01,  8.6495e-02,\n",
      "          2.3451e-02, -4.0903e-01,  5.5305e-03,  3.0992e-01, -4.4127e-01,\n",
      "          6.5249e-01,  3.3731e-01,  3.2748e-01, -2.4799e-01, -1.2766e-01,\n",
      "          8.4293e-02,  7.8142e-01, -3.2958e-01,  3.3641e-02,  1.1881e+00,\n",
      "         -2.7088e-01,  1.1844e+00, -2.5042e-01,  1.0545e+00, -5.9602e-01,\n",
      "         -4.9948e-01,  4.1983e-01, -7.7602e-03,  5.6827e-01, -2.4449e-01,\n",
      "         -4.5040e-01,  4.1491e-01, -3.6064e-01, -9.3222e-01,  8.6241e-01,\n",
      "         -8.6805e-01, -5.0062e-01, -6.2269e-01,  4.9958e-01, -3.3948e-01,\n",
      "         -1.4693e-01,  5.4371e-01,  5.0640e-01,  1.0244e-01,  7.6541e-01,\n",
      "          4.8737e-01,  6.3447e-01, -2.0920e-01,  2.0917e-01, -1.7359e-01,\n",
      "         -7.3389e-02, -2.7459e-01,  2.6353e-01,  4.0908e-01, -7.7818e-01,\n",
      "          9.0399e-01, -4.6443e-01,  1.4942e-01, -6.2836e-01, -1.3483e-01,\n",
      "         -1.3070e-02,  1.2684e-02, -7.9234e-01, -5.6172e-01,  2.0855e-01,\n",
      "         -3.3946e-01,  9.7982e-02,  8.4107e-01,  3.0211e-01, -6.6643e-01,\n",
      "         -4.8194e-01, -5.4834e-01, -2.1622e-01, -5.8302e-01, -3.9407e-01,\n",
      "         -4.9644e-01, -6.0656e-01,  3.8068e-01, -3.2252e-03, -6.4874e-01,\n",
      "          1.0255e+00,  1.7039e-01,  9.0351e-01, -2.3365e-01,  4.5488e-01,\n",
      "         -5.0627e-01,  4.8670e-01,  1.5132e-01,  1.6859e-01,  1.9607e-01,\n",
      "         -8.0886e-01, -2.7293e-01, -4.4382e-01,  1.1266e+00, -8.2087e-01,\n",
      "         -4.1948e-01,  4.5077e-01,  9.5946e-02,  2.7358e-01,  1.0293e-01,\n",
      "         -6.6057e-01,  1.6008e-01, -3.6037e-01,  9.7271e-01,  1.3151e-01,\n",
      "          5.2651e-01, -4.8892e-01, -6.9299e-01,  6.1456e-01,  1.3895e+00,\n",
      "          2.3061e-01, -1.9837e-01, -5.0869e-02, -6.1156e-01, -5.9961e-02,\n",
      "          8.7196e-02, -7.1910e-01,  1.1882e+00,  4.2363e-01, -2.2196e-01,\n",
      "         -4.8360e-01, -2.3663e-01, -4.9867e-01, -2.3766e-01,  1.0392e-01,\n",
      "         -2.4260e-01, -1.3717e-01,  9.4412e-01, -1.6571e-01, -2.7293e-02,\n",
      "         -6.9720e-01,  6.9499e-01,  4.6261e-01, -4.2053e-01, -8.3768e-01,\n",
      "         -1.1427e+00,  1.7640e-01, -1.0175e-01, -7.7167e-01, -2.5624e-01,\n",
      "         -1.0422e+00,  9.9877e-01,  4.5859e-01, -3.8014e-01, -4.0977e-01,\n",
      "          5.7929e-01,  2.7731e-01,  6.4518e-01,  8.2323e-01,  2.1841e-01,\n",
      "         -3.8692e-01, -7.2250e-02,  5.9777e-01,  8.2803e-02, -5.2651e-01,\n",
      "         -1.9122e-02,  7.6135e-02, -1.8668e-01, -4.3781e-01,  1.0035e+00,\n",
      "          5.0777e-01, -1.5537e-01, -4.4980e-01,  3.9199e-01,  7.9557e-02,\n",
      "         -4.6441e-01,  1.1246e+00,  3.7713e-02,  7.3869e-01, -1.0519e+00,\n",
      "         -1.0197e+00,  6.2874e-02,  6.4441e-01, -5.4479e-01,  2.0863e-01,\n",
      "          4.3825e-01, -1.1394e-01,  4.7492e-01, -4.9576e-01, -3.2381e-01,\n",
      "          4.5066e-01,  6.7590e-01, -9.5724e-03, -3.2899e-02, -3.8747e-01,\n",
      "         -2.8851e-01,  1.2416e-01,  3.0933e-01,  3.0374e-01,  1.7279e-01,\n",
      "         -6.1409e-02,  7.1220e-01,  2.3844e-01, -4.9143e-03,  3.1640e-01,\n",
      "         -2.3844e-01, -8.9093e-01, -3.4002e-01, -4.7567e-01,  4.9224e-01,\n",
      "          2.9702e-01,  1.6058e-01, -3.0287e-01,  9.2532e-01,  3.6964e-01,\n",
      "         -3.7914e-01,  3.6281e-01, -1.6869e-01,  1.9553e-01, -3.7585e-01,\n",
      "          4.3782e-01,  1.2483e+00,  2.5969e-02,  7.0210e-01,  6.3558e-01,\n",
      "          2.8685e-01,  7.9970e-01, -5.6859e-01,  6.9187e-01, -6.1191e-01,\n",
      "          1.9611e-01,  7.3584e-01,  3.2051e-02,  1.4809e-01, -5.6904e-01,\n",
      "         -7.4447e-01,  7.0111e-01, -2.1779e-01, -4.8564e-01,  3.2942e-01,\n",
      "         -7.7362e-01,  8.2131e-01, -7.6516e-01,  6.0569e-02, -5.8934e-01,\n",
      "          2.4163e-02, -2.1153e-01,  1.1577e-01,  3.4459e-01,  9.4858e-02,\n",
      "          3.3278e-01,  2.3672e-01, -6.5691e-01, -2.4135e-01,  1.2155e-01,\n",
      "         -7.5883e-01, -1.1274e+00,  4.1594e-01,  8.9990e-01,  8.4137e-01,\n",
      "         -3.4832e-01, -5.7733e-01, -1.7556e-01,  2.8413e-01, -6.0859e-01,\n",
      "          4.0929e-01, -2.4358e-01,  1.7887e-02,  2.2350e-02,  1.4311e-02,\n",
      "         -4.8425e-02,  4.4004e-01,  5.0411e-02, -8.6216e-01,  2.9785e-01,\n",
      "         -1.7314e-01,  8.0813e-01, -1.4203e-01, -6.0674e-02, -7.2569e-01,\n",
      "         -2.7723e-01,  1.0740e+00, -9.1792e-01, -5.0234e-01,  8.1144e-01,\n",
      "         -5.8863e-02, -8.9135e-01,  2.2405e-02,  4.4241e-01,  5.3449e-01,\n",
      "          3.1199e-01, -1.8999e-01, -3.3482e-01,  1.6095e+00, -2.6642e-01,\n",
      "         -1.1618e+00, -8.6946e-02, -4.2068e-03,  4.1980e-01,  4.4974e-01,\n",
      "         -1.0091e+00, -1.3868e+00, -2.6878e-01, -4.6823e-01,  3.8929e-01,\n",
      "          3.2053e-01,  1.8838e-01, -2.2602e-01, -4.0238e-01, -5.3647e-01,\n",
      "         -1.6583e-01, -4.0510e-01,  4.6035e-01, -4.0236e-01,  4.7110e-01,\n",
      "          9.8640e-01,  3.0505e-01,  1.3988e+00,  1.8848e-01,  3.7899e-01,\n",
      "         -7.8618e-01, -2.8452e-01, -1.6939e-01,  2.6592e-01,  2.0847e-02,\n",
      "         -1.3460e-01, -2.6164e-01,  7.4420e-01,  3.0188e-02, -6.4480e-01,\n",
      "          1.8653e-01,  2.3167e-01,  8.2253e-01, -5.4918e-01, -1.2210e-01,\n",
      "         -1.2728e-02,  2.9731e-01,  5.0998e-01,  3.3829e-01,  4.3578e-01,\n",
      "          2.2289e-01, -5.3407e-01,  7.8616e-01,  4.9658e-01,  3.8387e-01,\n",
      "          5.4428e-01,  3.0835e-01, -1.7933e-01,  8.9555e-02,  5.7119e-01,\n",
      "         -1.2704e-01,  1.3114e-01, -4.1790e-01, -5.1504e-03,  4.1853e-01,\n",
      "         -7.3576e-01, -3.3677e-01, -8.3465e-01,  1.3764e+00, -6.1896e-01,\n",
      "         -1.9670e-02,  3.3889e-01,  3.6699e-01, -1.4523e-01,  5.2734e-02,\n",
      "         -2.5106e-01,  3.8068e-01, -5.4103e-01, -1.0073e+00,  1.5104e+00,\n",
      "          1.2551e+00, -3.8102e-01,  5.4103e-01, -9.1178e-01,  5.4198e-01,\n",
      "         -1.0676e+00, -1.0861e+00, -1.3334e-01,  8.0093e-01, -7.4361e-01,\n",
      "         -6.1959e-01, -1.6244e-01,  3.1689e-02,  1.2383e+00, -1.0160e+00,\n",
      "         -8.0072e-01,  3.0473e-01, -2.7588e-01,  6.0792e-01, -4.6741e-02,\n",
      "          9.2754e-02, -6.3913e-01, -1.4267e+00, -8.8519e-01, -5.2209e-01,\n",
      "          5.4637e-01,  3.2398e-01, -5.1148e-01,  1.4830e-01,  8.6120e-01,\n",
      "          1.4023e+00, -1.0533e-01,  5.4125e-01,  5.3335e-01, -4.6670e-01,\n",
      "         -6.4116e-01, -3.7757e-01, -1.3461e+00, -6.0587e-01, -2.9994e-01,\n",
      "          4.8182e-01,  8.8985e-02,  4.1038e-01, -4.9925e-01, -2.3015e-01,\n",
      "          1.4792e-01, -1.8756e-01, -4.8062e-01,  2.5680e-02, -9.4454e-01,\n",
      "         -2.4988e-01,  5.6673e-01, -1.4464e-01, -8.4832e-01,  5.5894e-02,\n",
      "         -2.6498e-01, -1.0250e+00,  5.3792e-01, -1.0252e-01, -1.3473e+00,\n",
      "         -4.3883e-02, -7.8731e-01,  5.2529e-01,  3.8092e-01, -2.4967e-02,\n",
      "          6.9348e-01, -2.2422e-01,  4.0728e-01,  3.3664e-01,  3.7040e-01,\n",
      "         -2.3382e-01,  5.8083e-01, -8.8174e-01, -6.2710e-01,  2.0687e-01,\n",
      "          1.4820e-01,  9.3590e-02,  3.3696e-01, -2.1295e-01, -2.3472e-01,\n",
      "         -2.3464e-01,  6.0446e-01, -5.6529e-01, -7.0619e-01, -2.4426e-03,\n",
      "          9.1226e-01, -3.0277e-01,  5.5126e-01, -8.4347e-01,  9.1725e-01,\n",
      "         -8.3610e-01,  1.2381e+00,  1.1507e+00, -4.3976e-01,  8.1625e-02,\n",
      "         -3.8862e-01,  6.6921e-02,  2.1866e-01,  9.8179e-01, -3.9153e-01,\n",
      "          3.4589e-01,  3.2992e-01, -5.5138e-02, -9.3786e-04, -6.1260e-01,\n",
      "         -2.0367e-01,  4.2196e-01,  1.4758e+00, -1.4988e-01,  6.3857e-01,\n",
      "          3.8817e-01,  8.0837e-02, -4.3448e-01, -7.8176e-01,  2.5764e-01,\n",
      "          1.0928e+00, -7.3274e-02, -2.8246e-01, -1.0993e+00,  3.8420e-02,\n",
      "         -7.8798e-01, -8.9186e-01, -1.4706e+00, -8.8592e-01,  3.5703e-01,\n",
      "         -8.1422e-01,  6.6155e-02, -8.4585e-01,  3.7058e-01, -1.3117e+00,\n",
      "          9.1444e-01, -1.4100e-02, -8.2522e-01,  4.0200e-01, -1.0002e+00,\n",
      "         -1.0962e+00, -1.1699e+00,  3.1665e-01, -6.6594e-01, -1.9850e-01,\n",
      "          1.2348e+00, -9.3473e-01,  4.0010e-01, -2.7386e-01, -1.4739e-01,\n",
      "          8.3689e-01, -7.5579e-02,  3.9941e-01, -6.6749e-01, -1.0228e-01,\n",
      "          1.6492e+00, -8.9082e-01,  4.1760e-01, -8.0125e-01,  5.1736e-01,\n",
      "          5.7276e-01, -1.3763e-01, -1.4154e-01,  2.2499e-01,  3.8763e-01,\n",
      "         -3.5704e-01, -3.1824e-01, -1.0829e+00, -4.4137e-01, -5.1421e-02,\n",
      "          9.0120e-01,  8.5302e-01,  4.7168e-02, -1.0309e+00, -8.8628e-01,\n",
      "          7.0194e-01, -1.5639e-02,  3.0497e-01,  7.5383e-02, -1.0633e+00,\n",
      "          3.4580e-01, -7.7381e-01, -1.4603e-01, -5.7459e-01,  3.9303e-01,\n",
      "         -3.7377e-01, -8.7163e-01,  1.8724e-01, -2.3026e-01,  1.4582e-01,\n",
      "         -7.8474e-01, -1.3267e-01,  7.5646e-01, -7.4605e-01, -5.8356e-01,\n",
      "          3.8203e-02,  6.4920e-01, -2.1245e-02,  8.5895e-01,  9.0142e-01,\n",
      "         -3.4828e-01,  1.1549e+00, -3.4097e-01, -1.9045e-01, -3.4531e-01,\n",
      "         -4.0587e-01, -5.5754e-02, -2.7855e-01,  3.0966e-01,  1.0251e+00,\n",
      "          4.1392e-01, -3.8943e-02, -1.6935e-01,  7.1291e-03, -5.2153e-01,\n",
      "          1.5316e-01,  4.2744e-02,  3.2440e-01, -2.0195e-01, -3.0984e-01,\n",
      "          2.1694e-01,  9.7170e-01,  5.1282e-01,  5.3841e-01,  1.5173e-01,\n",
      "          5.5549e-01,  5.6463e-01, -3.7390e-01, -1.4389e-01, -8.9144e-02,\n",
      "         -2.0914e-01,  7.0052e-01,  5.8372e-01, -1.3116e+00, -3.7528e-01,\n",
      "          1.5656e-02,  5.2929e-01,  4.2169e-02, -1.7751e-01,  5.1430e-02,\n",
      "         -5.6771e-02,  7.6726e-01,  1.1676e+00, -5.6180e-01, -5.3962e-02,\n",
      "          1.1010e-01, -2.6692e-02, -3.1485e-01,  4.0389e-01, -2.9264e-01,\n",
      "          4.2181e-01,  3.2317e-01, -9.9187e-03,  3.3709e-01,  3.5549e-01,\n",
      "          7.8016e-01,  2.2197e-01,  1.1366e+00,  1.9821e-01, -3.2620e-01,\n",
      "          3.0186e-01, -6.8982e-01,  1.0979e+00,  1.3000e-01, -1.7920e-01,\n",
      "          6.6253e-01, -5.0248e-01, -2.0186e-01, -3.5330e-01, -8.0541e-01,\n",
      "         -9.7898e-01, -7.5083e-01, -3.3120e-01, -5.8682e-01, -4.5285e-01,\n",
      "         -1.4472e+00,  2.9955e-01, -3.5833e-01, -4.3919e-01,  1.0783e+00,\n",
      "          6.0711e-01,  2.2267e-01, -5.6093e-02, -6.5937e-01,  4.1429e-01,\n",
      "         -4.1862e-01, -2.2328e-01,  1.0049e-01, -5.5225e-01,  3.5043e-01,\n",
      "          6.1147e-01,  7.3914e-03, -3.9348e-01,  1.2029e+00,  3.2938e-01,\n",
      "         -4.9244e-01,  8.6248e-01, -3.6627e-01, -2.8320e-01,  6.1413e-01,\n",
      "          5.1906e-01, -1.8153e-01,  4.1530e-01,  5.4988e-01,  4.3267e-01,\n",
      "          7.1105e-01, -5.9523e-02,  3.6837e-01, -3.6972e-01, -8.3489e-01,\n",
      "         -2.5278e-01, -7.7540e-01, -1.3573e+00,  1.4270e-01,  6.4550e-01,\n",
      "          3.7768e-01, -9.3948e-02,  7.2665e-01, -9.0414e-01,  1.9155e+00,\n",
      "         -9.4037e-01, -1.1277e+00,  2.5667e-01,  2.1882e-01, -3.2515e-01,\n",
      "          4.9409e-01,  1.2327e+00,  1.2582e+00,  3.7731e-01,  6.1302e-01,\n",
      "          1.9056e+00,  4.3373e-01, -7.6703e-01,  3.7638e-01, -4.2204e-01,\n",
      "          2.7986e-01, -3.3823e-01, -8.4078e-01,  9.3521e-01,  4.8782e-01,\n",
      "         -4.6027e-01,  7.2608e-01, -7.4943e-01, -1.3453e+00,  4.0384e-01,\n",
      "          4.4063e-02,  1.4708e-01,  5.2566e-02,  6.9575e-01, -2.5489e-01,\n",
      "         -2.2412e-02,  2.8319e-01, -2.6196e-01, -1.6363e-01, -5.4874e-01,\n",
      "         -1.1225e-01, -6.6437e-01,  3.2121e-01]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.2889, -1.1408, -0.0958,  ..., -0.0722, -0.3775,  0.0928],\n",
      "        [ 0.0343, -0.7729, -0.6344,  ..., -0.2496, -0.7328, -0.2469],\n",
      "        [-0.2579, -0.7443, -0.3649,  ..., -0.1908, -0.6371, -0.0613],\n",
      "        [ 0.3077, -1.0279, -0.0817,  ...,  0.8408, -0.6206,  0.5691]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)], predicted_segment_bboxes=[[], tensor([[ 0.0579,  0.5522, -0.9517,  0.3199]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.1314,  0.6634, -0.5705,  0.0275],\n",
      "        [ 0.0397,  1.2902, -0.7337, -0.1688],\n",
      "        [ 0.0996,  1.1790, -0.7800, -0.1269],\n",
      "        [-0.1945, -0.0652, -0.5900, -0.5263]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer([\n",
    "    'Hellow world!',\n",
    "    'This is a segment token: <|startofimage|><|seg|><|endofimage|><|endoftext|>',\n",
    "    'This is an image of a cat with 4 tokens: <|startofimage|><|seg|><|seg|><|seg|><|seg|><|endofimage|>!<|endoftext|>',\n",
    "    ], return_tensors=\"pt\", return_attention_mask=False, padding=True).to(DEVICE)\n",
    "\n",
    "print(tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=False))\n",
    "print(tokenizer.decode(inputs['input_ids'][1], skip_special_tokens=False))\n",
    "print(tokenizer.decode(inputs['input_ids'][2], skip_special_tokens=False))\n",
    "\n",
    "print(inputs, inputs['input_ids'].shape)\n",
    "\n",
    "print(model.special_token_id_mappinmg)\n",
    "print('\\n\\n\\n')\n",
    "output = model(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    segment_masks=[[], segment_masks[:1], segment_masks[:4]],\n",
    "    images=images[:4],\n",
    "    labels=inputs['input_ids'],\n",
    "    )\n",
    "print(output)\n",
    "\n",
    "# outputs = model.generate(**inputs, max_length=200)\n",
    "# text = tokenizer.batch_decode(outputs)[0]\n",
    "# print(text)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq2seq-ae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
